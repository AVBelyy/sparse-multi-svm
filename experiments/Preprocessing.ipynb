{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import dok_matrix, csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = (\"LSHTC1\", \"DMOZ\", \"WIKI_Small\", \"WIKI_50K\", \"WIKI_100K\")\n",
    "dataset_dir = \"../data\"\n",
    "out_dir = \"../data/parsed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset WIKI_Small\n",
      "\ttrain size: (796617, 380078)\n",
      "\ttest size: (199155, 380078)\n",
      "\theldout size: (5000, 380078)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [08:44<17:29, 524.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset WIKI_50K\n",
      "\ttrain size: (1102754, 951558)\n",
      "\ttest size: (276939, 951558)\n",
      "\theldout size: (5000, 951558)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [21:49<10:54, 654.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset WIKI_100K\n",
      "\ttrain size: (2195530, 1271710)\n",
      "\ttest size: (550133, 1271710)\n",
      "\theldout size: (5000, 1271710)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [50:56<00:00, 1018.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43min 24s, sys: 4min 45s, total: 48min 10s\n",
      "Wall time: 50min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Write features\n",
    "\n",
    "for dataset_name in tqdm(datasets_names):\n",
    "    dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "    train_name, test_name, heldout_name = \"bikash_train_remapped.tf\", \"bikash_test_remapped.tf\", \\\n",
    "                                          \"bikash_heldout_remapped.tf\"\n",
    "\n",
    "    docs_cnts = collections.defaultdict(int)\n",
    "    print(\"Dataset %s\" % dataset_name)\n",
    "    words_cnt = 0\n",
    "    # Count number of docs and words\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        data_path = os.path.join(dataset_path, \"bikash_%s_remapped.tf\" % data_name)\n",
    "        with open(data_path) as fin:\n",
    "            for doc in fin:\n",
    "                tokens_cnts = [int(x.split(\":\")[0]) for x in doc.split()[1:]]\n",
    "                words_cnt = max(words_cnt, max(tokens_cnts) + 1)\n",
    "                docs_cnts[data_name] += 1\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        print(\"\\t%s size: (%d, %d)\" % (data_name, docs_cnts[data_name], words_cnt))\n",
    "    # Create sparse matrices\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        data = dok_matrix((docs_cnts[data_name], words_cnt), dtype=np.int32)\n",
    "        data_path = os.path.join(dataset_path, \"bikash_%s_remapped.tf\" % data_name)\n",
    "        with open(data_path) as fin:\n",
    "            for i, doc in enumerate(fin):\n",
    "                for token_cnt_str in doc.split()[1:]:\n",
    "                    j, cnt = map(int, token_cnt_str.split(\":\"))\n",
    "                    data[i, j] = cnt\n",
    "        out_path = os.path.join(out_dir, \"%s_%s.dump\" % (dataset_name, data_name))\n",
    "        with open(out_path, \"wb\") as fout:\n",
    "            pickle.dump(csr_matrix(data), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_names = (\"DMOZ\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset DMOZ\n",
      "\tclasses count: 27875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.3 s, sys: 653 ms, total: 13 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Write labels\n",
    "\n",
    "for dataset_name in tqdm(datasets_names):\n",
    "    dataset_path = os.path.join(dataset_dir, dataset_name)\n",
    "    train_name, test_name, heldout_name = \"bikash_train_remapped.tf\", \"bikash_test_remapped.tf\", \\\n",
    "                                          \"bikash_heldout_remapped.tf\"\n",
    "\n",
    "    classes = {}\n",
    "    print(\"Dataset %s\" % dataset_name)\n",
    "    # Count number of docs and words\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        data_path = os.path.join(dataset_path, \"bikash_%s_remapped.tf\" % data_name)\n",
    "        with open(data_path) as fin:\n",
    "            for doc in fin:\n",
    "                some_id = int(doc.split()[0])\n",
    "                class_id = classes.get(some_id, len(classes))\n",
    "                classes[some_id] = class_id\n",
    "    print(\"\\tclasses count: %d\" % len(classes))\n",
    "    # Create arrays with labels\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        labels = []\n",
    "        data_path = os.path.join(dataset_path, \"bikash_%s_remapped.tf\" % data_name)\n",
    "        with open(data_path) as fin:\n",
    "            for doc in fin:\n",
    "                some_id = int(doc.split()[0])\n",
    "                class_id = classes[some_id]\n",
    "                labels.append(class_id)\n",
    "        out_path = os.path.join(out_dir, \"%s_%s_out.dump\" % (dataset_name, data_name))\n",
    "        with open(out_path, \"wb\") as fout:\n",
    "            pickle.dump(np.array(labels), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33, 212: train (849, 824)\n",
    "# 33, 212: test (220, 198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1927, 2226: train (903, 903)\n",
    "# 1927, 2226: test (222, 221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to pickle protocol=2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 754 ms, sys: 996 ms, total: 1.75 s\n",
      "Wall time: 2.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert pickle protocol=3 to pickle protocol=2 for python2.\n",
    "\n",
    "for dataset_name in datasets_names:\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        data3_path = os.path.join(out_dir, \"%s_%s.dump\" % (dataset_name, data_name))\n",
    "        data3_out_path = os.path.join(out_dir, \"%s_%s_out.dump\" % (dataset_name, data_name))\n",
    "        data2_path = os.path.join(out_dir, \"%s_%s_py2.dump\" % (dataset_name, data_name))\n",
    "        data2_out_path = os.path.join(out_dir, \"%s_%s_py2_out.dump\" % (dataset_name, data_name))\n",
    "        with open(data3_path, \"rb\") as fin, open(data2_path, \"wb\") as fout:\n",
    "            m = pickle.load(fin)\n",
    "            pickle.dump(m, fout, protocol=2)\n",
    "        with open(data3_out_path, \"rb\") as fin, open(data2_out_path, \"wb\") as fout:\n",
    "            m = pickle.load(fin)\n",
    "            pickle.dump(m, fout, protocol=2)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion to SVM-light format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 39s, sys: 4.06 s, total: 3min 44s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Convert pickle protocol=3 to SVM-light format.\n",
    "\n",
    "pos_label = 1927\n",
    "neg_label = 2226\n",
    "\n",
    "for dataset_name in datasets_names[::-1]:\n",
    "    for data_name in (\"train\", \"test\", \"heldout\"):\n",
    "        data_path = os.path.join(out_dir, \"%s_%s.dump\" % (dataset_name, data_name))\n",
    "        data_out_path = os.path.join(out_dir, \"%s_%s_out.dump\" % (dataset_name, data_name))\n",
    "        svmlight_path = os.path.join(out_dir, \"%s_%s_svmlight.txt\" % (dataset_name, data_name))\n",
    "        with open(data_path, \"rb\") as fin:\n",
    "            X = pickle.load(fin)\n",
    "        with open(data_out_path, \"rb\") as fin:\n",
    "            y = pickle.load(fin)\n",
    "        # Uncommented is one-vs-one,\n",
    "        # Commented is one-vs-rest\n",
    "        # X = X[(y == pos_label) | (y == neg_label)]\n",
    "        # y = y[(y == pos_label) | (y == neg_label)]\n",
    "        X.sort_indices()\n",
    "        with open(svmlight_path, \"w\") as fout:\n",
    "            for obj, label in zip(X, y):\n",
    "                obj_str = \" \".join(map(lambda p: \"%d:%d\" % p, zip(obj.indices, obj.data)))\n",
    "                label_str = \"+1\" if label == pos_label else \"-1\"\n",
    "                fout.write(\"%s %s\\n\" % (label_str, obj_str))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
